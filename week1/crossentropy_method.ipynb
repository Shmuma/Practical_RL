{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crossentropy method\n",
    "\n",
    "This notebook will teach you to solve reinforcement learning with crossentropy method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XVFB will be launched if you run on a server\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-10 20:10:20,488] Making new env: Taxi-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "env = gym.make(\"Taxi-v2\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_states=500, n_actions=6\n"
     ]
    }
   ],
   "source": [
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"n_states=%i, n_actions=%i\"%(n_states,n_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create stochastic policy\n",
    "\n",
    "This time our policy should be a probability distribution.\n",
    "\n",
    "```policy[s,a] = P(take action a | in state s)```\n",
    "\n",
    "Since we still use integer state and action representations, you can use a 2-dimensional array to represent the policy.\n",
    "\n",
    "Please initialize policy __uniformly__, that is, probabililities of all actions should be equal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policy = np.ones(shape=(n_states, n_actions)) / n_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert type(policy) in (np.ndarray,np.matrix)\n",
    "assert np.allclose(policy,1./n_actions)\n",
    "assert np.allclose(np.sum(policy,axis=1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play the game\n",
    "\n",
    "Just like before, but we also record all states and actions we took."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_session(t_max=10**4):\n",
    "    \"\"\"\n",
    "    Play game until end or for t_max ticks.\n",
    "    returns: list of states, list of actions and sum of rewards\n",
    "    \"\"\"\n",
    "    states,actions = [],[]\n",
    "    total_reward = 0.\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):        \n",
    "        a = np.random.choice(n_actions, p=policy[s])\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward += r\n",
    "        \n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "    return states,actions,total_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s,a,r = generate_session()\n",
    "assert type(s) == type(a) == list\n",
    "assert len(s) == len(a)\n",
    "assert type(r) is float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "Generate sessions, select N best and fit to those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.49 s, sys: 16 ms, total: 2.51 s\n",
      "Wall time: 2.5 s\n",
      "mean reward = -761.60400\tthreshold = -776.0\n",
      "CPU times: user 2.39 s, sys: 4 ms, total: 2.39 s\n",
      "Wall time: 2.38 s\n",
      "mean reward = -697.53600\tthreshold = -731.0\n",
      "CPU times: user 2.34 s, sys: 4 ms, total: 2.34 s\n",
      "Wall time: 2.34 s\n",
      "mean reward = -651.58400\tthreshold = -677.0\n",
      "CPU times: user 2.37 s, sys: 4 ms, total: 2.37 s\n",
      "Wall time: 2.36 s\n",
      "mean reward = -578.94400\tthreshold = -614.0\n",
      "CPU times: user 2.17 s, sys: 4 ms, total: 2.17 s\n",
      "Wall time: 2.17 s\n",
      "mean reward = -495.73200\tthreshold = -551.0\n",
      "CPU times: user 2.02 s, sys: 8 ms, total: 2.03 s\n",
      "Wall time: 2.02 s\n",
      "mean reward = -425.86000\tthreshold = -470.0\n",
      "CPU times: user 1.61 s, sys: 4 ms, total: 1.61 s\n",
      "Wall time: 1.6 s\n",
      "mean reward = -303.87600\tthreshold = -309.0\n",
      "CPU times: user 1.24 s, sys: 4 ms, total: 1.24 s\n",
      "Wall time: 1.24 s\n",
      "mean reward = -231.20000\tthreshold = -195.0\n",
      "CPU times: user 868 ms, sys: 4 ms, total: 872 ms\n",
      "Wall time: 872 ms\n",
      "mean reward = -142.61200\tthreshold = -117.5\n",
      "CPU times: user 752 ms, sys: 4 ms, total: 756 ms\n",
      "Wall time: 742 ms\n",
      "mean reward = -112.92000\tthreshold = -88.5\n",
      "CPU times: user 620 ms, sys: 0 ns, total: 620 ms\n",
      "Wall time: 611 ms\n",
      "mean reward = -91.18400\tthreshold = -58.5\n",
      "CPU times: user 500 ms, sys: 4 ms, total: 504 ms\n",
      "Wall time: 496 ms\n",
      "mean reward = -68.60400\tthreshold = -47.0\n",
      "CPU times: user 492 ms, sys: 8 ms, total: 500 ms\n",
      "Wall time: 488 ms\n",
      "mean reward = -64.95200\tthreshold = -35.0\n",
      "CPU times: user 472 ms, sys: 20 ms, total: 492 ms\n",
      "Wall time: 478 ms\n",
      "mean reward = -60.40000\tthreshold = -36.5\n",
      "CPU times: user 404 ms, sys: 12 ms, total: 416 ms\n",
      "Wall time: 407 ms\n",
      "mean reward = -46.37600\tthreshold = -24.0\n",
      "CPU times: user 472 ms, sys: 8 ms, total: 480 ms\n",
      "Wall time: 469 ms\n",
      "mean reward = -71.46400\tthreshold = -22.0\n",
      "CPU times: user 464 ms, sys: 4 ms, total: 468 ms\n",
      "Wall time: 460 ms\n",
      "mean reward = -61.55200\tthreshold = -14.0\n",
      "CPU times: user 476 ms, sys: 8 ms, total: 484 ms\n",
      "Wall time: 480 ms\n",
      "mean reward = -67.47600\tthreshold = -18.0\n",
      "CPU times: user 532 ms, sys: 16 ms, total: 548 ms\n",
      "Wall time: 542 ms\n",
      "mean reward = -80.14400\tthreshold = -17.0\n",
      "CPU times: user 528 ms, sys: 8 ms, total: 536 ms\n",
      "Wall time: 527 ms\n",
      "mean reward = -93.24000\tthreshold = -22.0\n",
      "CPU times: user 576 ms, sys: 4 ms, total: 580 ms\n",
      "Wall time: 573 ms\n",
      "mean reward = -114.68800\tthreshold = -22.0\n",
      "CPU times: user 428 ms, sys: 0 ns, total: 428 ms\n",
      "Wall time: 419 ms\n",
      "mean reward = -60.27200\tthreshold = -12.0\n",
      "CPU times: user 548 ms, sys: 0 ns, total: 548 ms\n",
      "Wall time: 539 ms\n",
      "mean reward = -102.40800\tthreshold = -16.0\n",
      "CPU times: user 560 ms, sys: 0 ns, total: 560 ms\n",
      "Wall time: 548 ms\n",
      "mean reward = -105.04800\tthreshold = -17.5\n",
      "CPU times: user 508 ms, sys: 0 ns, total: 508 ms\n",
      "Wall time: 503 ms\n",
      "mean reward = -90.55600\tthreshold = -17.5\n",
      "CPU times: user 512 ms, sys: 8 ms, total: 520 ms\n",
      "Wall time: 509 ms\n",
      "mean reward = -92.48000\tthreshold = -19.0\n",
      "CPU times: user 456 ms, sys: 4 ms, total: 460 ms\n",
      "Wall time: 448 ms\n",
      "mean reward = -74.35200\tthreshold = -16.5\n",
      "CPU times: user 464 ms, sys: 8 ms, total: 472 ms\n",
      "Wall time: 465 ms\n",
      "mean reward = -76.47600\tthreshold = -17.0\n",
      "CPU times: user 532 ms, sys: 12 ms, total: 544 ms\n",
      "Wall time: 530 ms\n",
      "mean reward = -94.98400\tthreshold = -15.0\n",
      "CPU times: user 560 ms, sys: 12 ms, total: 572 ms\n",
      "Wall time: 563 ms\n",
      "mean reward = -111.94000\tthreshold = -11.0\n",
      "CPU times: user 476 ms, sys: 8 ms, total: 484 ms\n",
      "Wall time: 472 ms\n",
      "mean reward = -74.52000\tthreshold = -10.5\n",
      "CPU times: user 588 ms, sys: 8 ms, total: 596 ms\n",
      "Wall time: 592 ms\n",
      "mean reward = -115.28800\tthreshold = -14.0\n",
      "CPU times: user 572 ms, sys: 0 ns, total: 572 ms\n",
      "Wall time: 564 ms\n",
      "mean reward = -93.91600\tthreshold = -11.0\n",
      "CPU times: user 544 ms, sys: 4 ms, total: 548 ms\n",
      "Wall time: 543 ms\n",
      "mean reward = -91.45200\tthreshold = -12.5\n",
      "CPU times: user 456 ms, sys: 4 ms, total: 460 ms\n",
      "Wall time: 450 ms\n",
      "mean reward = -62.82800\tthreshold = -10.5\n",
      "CPU times: user 508 ms, sys: 12 ms, total: 520 ms\n",
      "Wall time: 504 ms\n",
      "mean reward = -85.00000\tthreshold = -9.0\n",
      "CPU times: user 564 ms, sys: 24 ms, total: 588 ms\n",
      "Wall time: 576 ms\n",
      "mean reward = -103.10800\tthreshold = -13.0\n",
      "CPU times: user 612 ms, sys: 4 ms, total: 616 ms\n",
      "Wall time: 610 ms\n",
      "mean reward = -109.72400\tthreshold = -13.0\n",
      "CPU times: user 480 ms, sys: 4 ms, total: 484 ms\n",
      "Wall time: 476 ms\n",
      "mean reward = -80.09600\tthreshold = -11.0\n",
      "CPU times: user 660 ms, sys: 8 ms, total: 668 ms\n",
      "Wall time: 650 ms\n",
      "mean reward = -135.40400\tthreshold = -13.0\n",
      "CPU times: user 576 ms, sys: 4 ms, total: 580 ms\n",
      "Wall time: 566 ms\n",
      "mean reward = -99.34800\tthreshold = -16.0\n",
      "CPU times: user 520 ms, sys: 4 ms, total: 524 ms\n",
      "Wall time: 519 ms\n",
      "mean reward = -90.94400\tthreshold = -14.0\n",
      "CPU times: user 552 ms, sys: 4 ms, total: 556 ms\n",
      "Wall time: 551 ms\n",
      "mean reward = -99.41200\tthreshold = -16.0\n",
      "CPU times: user 564 ms, sys: 8 ms, total: 572 ms\n",
      "Wall time: 569 ms\n",
      "mean reward = -99.90400\tthreshold = -18.0\n",
      "CPU times: user 532 ms, sys: 4 ms, total: 536 ms\n",
      "Wall time: 530 ms\n",
      "mean reward = -93.40000\tthreshold = -16.0\n",
      "CPU times: user 560 ms, sys: 8 ms, total: 568 ms\n",
      "Wall time: 555 ms\n",
      "mean reward = -95.43600\tthreshold = -15.5\n",
      "CPU times: user 520 ms, sys: 8 ms, total: 528 ms\n",
      "Wall time: 521 ms\n",
      "mean reward = -94.64000\tthreshold = -14.0\n",
      "CPU times: user 588 ms, sys: 0 ns, total: 588 ms\n",
      "Wall time: 578 ms\n",
      "mean reward = -109.95200\tthreshold = -15.0\n",
      "CPU times: user 508 ms, sys: 28 ms, total: 536 ms\n",
      "Wall time: 521 ms\n",
      "mean reward = -88.46000\tthreshold = -9.0\n",
      "CPU times: user 576 ms, sys: 24 ms, total: 600 ms\n",
      "Wall time: 582 ms\n",
      "mean reward = -105.28800\tthreshold = -13.5\n",
      "CPU times: user 524 ms, sys: 16 ms, total: 540 ms\n",
      "Wall time: 527 ms\n",
      "mean reward = -89.88000\tthreshold = -13.0\n",
      "CPU times: user 468 ms, sys: 0 ns, total: 468 ms\n",
      "Wall time: 458 ms\n",
      "mean reward = -67.69200\tthreshold = -11.0\n",
      "CPU times: user 544 ms, sys: 8 ms, total: 552 ms\n",
      "Wall time: 542 ms\n",
      "mean reward = -100.17600\tthreshold = -8.5\n",
      "CPU times: user 588 ms, sys: 0 ns, total: 588 ms\n",
      "Wall time: 578 ms\n",
      "mean reward = -112.74400\tthreshold = -18.0\n",
      "CPU times: user 556 ms, sys: 4 ms, total: 560 ms\n",
      "Wall time: 543 ms\n",
      "mean reward = -95.46400\tthreshold = -13.0\n",
      "CPU times: user 520 ms, sys: 4 ms, total: 524 ms\n",
      "Wall time: 515 ms\n",
      "mean reward = -87.68000\tthreshold = -12.0\n",
      "CPU times: user 560 ms, sys: 4 ms, total: 564 ms\n",
      "Wall time: 560 ms\n",
      "mean reward = -109.62800\tthreshold = -15.0\n",
      "CPU times: user 528 ms, sys: 16 ms, total: 544 ms\n",
      "Wall time: 530 ms\n",
      "mean reward = -96.80000\tthreshold = -13.0\n",
      "CPU times: user 636 ms, sys: 8 ms, total: 644 ms\n",
      "Wall time: 636 ms\n",
      "mean reward = -130.72800\tthreshold = -10.5\n",
      "CPU times: user 620 ms, sys: 8 ms, total: 628 ms\n",
      "Wall time: 621 ms\n",
      "mean reward = -125.70800\tthreshold = -11.5\n",
      "CPU times: user 544 ms, sys: 4 ms, total: 548 ms\n",
      "Wall time: 537 ms\n",
      "mean reward = -96.29600\tthreshold = -13.0\n",
      "CPU times: user 636 ms, sys: 4 ms, total: 640 ms\n",
      "Wall time: 628 ms\n",
      "mean reward = -132.96800\tthreshold = -18.0\n",
      "CPU times: user 596 ms, sys: 4 ms, total: 600 ms\n",
      "Wall time: 585 ms\n",
      "mean reward = -112.81200\tthreshold = -12.5\n",
      "CPU times: user 608 ms, sys: 8 ms, total: 616 ms\n",
      "Wall time: 600 ms\n",
      "mean reward = -105.70400\tthreshold = -14.0\n",
      "CPU times: user 532 ms, sys: 16 ms, total: 548 ms\n",
      "Wall time: 535 ms\n",
      "mean reward = -93.32000\tthreshold = -14.0\n",
      "CPU times: user 540 ms, sys: 0 ns, total: 540 ms\n",
      "Wall time: 535 ms\n",
      "mean reward = -95.89200\tthreshold = -15.0\n",
      "CPU times: user 508 ms, sys: 8 ms, total: 516 ms\n",
      "Wall time: 504 ms\n",
      "mean reward = -89.28000\tthreshold = -10.5\n",
      "CPU times: user 504 ms, sys: 12 ms, total: 516 ms\n",
      "Wall time: 500 ms\n",
      "mean reward = -84.45200\tthreshold = -15.0\n",
      "CPU times: user 496 ms, sys: 8 ms, total: 504 ms\n",
      "Wall time: 493 ms\n",
      "mean reward = -88.92800\tthreshold = -8.0\n",
      "CPU times: user 648 ms, sys: 0 ns, total: 648 ms\n",
      "Wall time: 638 ms\n",
      "mean reward = -138.54400\tthreshold = -18.0\n",
      "CPU times: user 476 ms, sys: 12 ms, total: 488 ms\n",
      "Wall time: 473 ms\n",
      "mean reward = -78.46400\tthreshold = -9.0\n",
      "CPU times: user 572 ms, sys: 4 ms, total: 576 ms\n",
      "Wall time: 565 ms\n",
      "mean reward = -106.50800\tthreshold = -11.5\n",
      "CPU times: user 488 ms, sys: 12 ms, total: 500 ms\n",
      "Wall time: 493 ms\n",
      "mean reward = -83.61200\tthreshold = -12.0\n",
      "CPU times: user 752 ms, sys: 8 ms, total: 760 ms\n",
      "Wall time: 745 ms\n",
      "mean reward = -160.37600\tthreshold = -18.0\n",
      "CPU times: user 492 ms, sys: 8 ms, total: 500 ms\n",
      "Wall time: 492 ms\n",
      "mean reward = -77.12400\tthreshold = -14.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 508 ms, sys: 16 ms, total: 524 ms\n",
      "Wall time: 506 ms\n",
      "mean reward = -80.67200\tthreshold = -14.0\n",
      "CPU times: user 400 ms, sys: 4 ms, total: 404 ms\n",
      "Wall time: 397 ms\n",
      "mean reward = -58.83600\tthreshold = -12.0\n",
      "CPU times: user 536 ms, sys: 16 ms, total: 552 ms\n",
      "Wall time: 542 ms\n",
      "mean reward = -106.18800\tthreshold = -18.0\n",
      "CPU times: user 476 ms, sys: 4 ms, total: 480 ms\n",
      "Wall time: 472 ms\n",
      "mean reward = -87.29600\tthreshold = -13.5\n",
      "CPU times: user 488 ms, sys: 4 ms, total: 492 ms\n",
      "Wall time: 478 ms\n",
      "mean reward = -82.46800\tthreshold = -14.0\n",
      "CPU times: user 460 ms, sys: 8 ms, total: 468 ms\n",
      "Wall time: 460 ms\n",
      "mean reward = -76.65200\tthreshold = -13.0\n",
      "CPU times: user 456 ms, sys: 0 ns, total: 456 ms\n",
      "Wall time: 446 ms\n",
      "mean reward = -76.64400\tthreshold = -7.0\n",
      "CPU times: user 420 ms, sys: 4 ms, total: 424 ms\n",
      "Wall time: 419 ms\n",
      "mean reward = -68.14000\tthreshold = -7.0\n",
      "CPU times: user 480 ms, sys: 12 ms, total: 492 ms\n",
      "Wall time: 485 ms\n",
      "mean reward = -91.53600\tthreshold = -12.0\n",
      "CPU times: user 532 ms, sys: 0 ns, total: 532 ms\n",
      "Wall time: 522 ms\n",
      "mean reward = -102.80400\tthreshold = -8.5\n",
      "CPU times: user 500 ms, sys: 0 ns, total: 500 ms\n",
      "Wall time: 494 ms\n",
      "mean reward = -93.46000\tthreshold = -11.5\n",
      "CPU times: user 728 ms, sys: 0 ns, total: 728 ms\n",
      "Wall time: 716 ms\n",
      "mean reward = -167.75600\tthreshold = -29.0\n",
      "CPU times: user 628 ms, sys: 4 ms, total: 632 ms\n",
      "Wall time: 627 ms\n",
      "mean reward = -127.30800\tthreshold = -21.5\n",
      "CPU times: user 512 ms, sys: 4 ms, total: 516 ms\n",
      "Wall time: 508 ms\n",
      "mean reward = -91.01200\tthreshold = -15.5\n",
      "CPU times: user 504 ms, sys: 8 ms, total: 512 ms\n",
      "Wall time: 502 ms\n",
      "mean reward = -89.94800\tthreshold = -14.0\n",
      "CPU times: user 556 ms, sys: 8 ms, total: 564 ms\n",
      "Wall time: 550 ms\n",
      "mean reward = -97.90800\tthreshold = -14.0\n",
      "CPU times: user 512 ms, sys: 0 ns, total: 512 ms\n",
      "Wall time: 504 ms\n",
      "mean reward = -80.18400\tthreshold = -11.0\n",
      "CPU times: user 452 ms, sys: 0 ns, total: 452 ms\n",
      "Wall time: 450 ms\n",
      "mean reward = -75.07600\tthreshold = -7.5\n",
      "CPU times: user 444 ms, sys: 16 ms, total: 460 ms\n",
      "Wall time: 448 ms\n",
      "mean reward = -70.07600\tthreshold = -6.5\n",
      "CPU times: user 548 ms, sys: 0 ns, total: 548 ms\n",
      "Wall time: 541 ms\n",
      "mean reward = -107.52400\tthreshold = -13.5\n",
      "CPU times: user 520 ms, sys: 0 ns, total: 520 ms\n",
      "Wall time: 517 ms\n",
      "mean reward = -99.76400\tthreshold = -14.5\n",
      "CPU times: user 496 ms, sys: 12 ms, total: 508 ms\n",
      "Wall time: 499 ms\n",
      "mean reward = -96.40800\tthreshold = -11.0\n",
      "CPU times: user 432 ms, sys: 4 ms, total: 436 ms\n",
      "Wall time: 429 ms\n",
      "mean reward = -70.32000\tthreshold = -9.0\n",
      "CPU times: user 416 ms, sys: 0 ns, total: 416 ms\n",
      "Wall time: 406 ms\n",
      "mean reward = -57.43600\tthreshold = -9.0\n",
      "CPU times: user 396 ms, sys: 12 ms, total: 408 ms\n",
      "Wall time: 398 ms\n",
      "mean reward = -55.51200\tthreshold = -14.0\n"
     ]
    }
   ],
   "source": [
    "n_samples = 250  #sample this many samples\n",
    "percentile = 50  #take this percent of session with highest rewards\n",
    "smoothing = 0.1  #add this thing to all counts for stability\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    %time sessions = [generate_session() for _ in range(n_samples)]\n",
    "\n",
    "    batch_states,batch_actions,batch_rewards = map(np.array,zip(*sessions))\n",
    "\n",
    "    #batch_states: a list of lists of states in each session\n",
    "    #batch_actions: a list of lists of actions in each session\n",
    "    #batch_rewards: a list of floats - total rewards at each session\n",
    "    \n",
    "    threshold = np.percentile(batch_rewards, percentile)\n",
    "    \n",
    "    elite_states = batch_states[batch_rewards >= threshold]\n",
    "    elite_actions = batch_actions[batch_rewards >= threshold]\n",
    "    \n",
    "    elite_states, elite_actions = map(np.concatenate,[elite_states,elite_actions])\n",
    "    #hint on task above: use np.percentile and numpy-style indexing\n",
    "    \n",
    "    #count actions from elite states\n",
    "    elite_counts = np.zeros_like(policy)+smoothing\n",
    "    \n",
    "    for s, a in zip(elite_states, elite_actions):\n",
    "        elite_counts[s][a] += 1\n",
    "\n",
    "    policy = elite_counts / elite_counts.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    \n",
    "    print(\"mean reward = %.5f\\tthreshold = %.1f\"%(np.mean(batch_rewards),threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate (deep) crossentropy method\n",
    "\n",
    "In this section we will train a neural network policy for continuous action space game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-10 20:12:02,673] Making new env: CartPole-v0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8c1f44e7f0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nS1KnDHhJ6tT/B5UakFMgXdeNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8c1f45ce80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shmuma/sfw/anaconda/envs/pract_rl/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#create agent\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "agent = MLPClassifier(hidden_layer_sizes=(20,20),\n",
    "                      activation='tanh',\n",
    "                      warm_start=True, #keep progress between .fit(...) calls\n",
    "                      max_iter=1 #make only 1 iteration on each .fit(...)\n",
    "                     )\n",
    "#initialize agent to the dimension of state an amount of actions\n",
    "agent.fit([env.reset()]*n_actions,range(n_actions));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000):\n",
    "    \n",
    "    states,actions = [],[]\n",
    "    total_reward = 0\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        #predict array of action probabilities\n",
    "        probs = agent.predict_proba([s])[0] \n",
    "        \n",
    "        a = np.random.choice(n_actions, p=probs)\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        #record sessions like you did before\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward+=r\n",
    "        \n",
    "        s = new_s\n",
    "        if done: break\n",
    "    return states,actions,total_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward = 20.51000\tthreshold = 22.0\n",
      "mean reward = 24.97000\tthreshold = 27.0\n",
      "mean reward = 24.72000\tthreshold = 25.3\n",
      "mean reward = 31.13000\tthreshold = 39.0\n",
      "mean reward = 33.45000\tthreshold = 39.0\n",
      "mean reward = 36.90000\tthreshold = 43.0\n",
      "mean reward = 45.53000\tthreshold = 54.0\n",
      "mean reward = 51.49000\tthreshold = 59.9\n",
      "mean reward = 56.43000\tthreshold = 64.6\n",
      "mean reward = 61.37000\tthreshold = 78.0\n",
      "mean reward = 72.53000\tthreshold = 89.0\n",
      "mean reward = 91.70000\tthreshold = 115.0\n",
      "mean reward = 100.82000\tthreshold = 118.0\n",
      "mean reward = 107.84000\tthreshold = 124.0\n",
      "mean reward = 114.91000\tthreshold = 131.3\n",
      "mean reward = 132.97000\tthreshold = 159.6\n",
      "mean reward = 138.38000\tthreshold = 166.6\n",
      "mean reward = 156.15000\tthreshold = 192.0\n",
      "mean reward = 162.32000\tthreshold = 200.0\n",
      "mean reward = 180.78000\tthreshold = 200.0\n",
      "mean reward = 188.59000\tthreshold = 200.0\n",
      "mean reward = 187.81000\tthreshold = 200.0\n",
      "mean reward = 191.23000\tthreshold = 200.0\n",
      "mean reward = 191.95000\tthreshold = 200.0\n",
      "mean reward = 192.15000\tthreshold = 200.0\n",
      "mean reward = 196.13000\tthreshold = 200.0\n",
      "mean reward = 193.87000\tthreshold = 200.0\n",
      "mean reward = 195.99000\tthreshold = 200.0\n",
      "mean reward = 196.32000\tthreshold = 200.0\n",
      "mean reward = 198.55000\tthreshold = 200.0\n",
      "mean reward = 198.05000\tthreshold = 200.0\n",
      "mean reward = 198.08000\tthreshold = 200.0\n",
      "mean reward = 199.69000\tthreshold = 200.0\n",
      "mean reward = 199.17000\tthreshold = 200.0\n",
      "mean reward = 199.66000\tthreshold = 200.0\n",
      "mean reward = 199.43000\tthreshold = 200.0\n",
      "mean reward = 199.87000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 198.06000\tthreshold = 200.0\n",
      "mean reward = 199.32000\tthreshold = 200.0\n",
      "mean reward = 199.42000\tthreshold = 200.0\n",
      "mean reward = 199.14000\tthreshold = 200.0\n",
      "mean reward = 199.05000\tthreshold = 200.0\n",
      "mean reward = 198.96000\tthreshold = 200.0\n",
      "mean reward = 199.94000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 199.70000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 199.95000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 199.82000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 199.99000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 198.79000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n",
      "mean reward = 199.56000\tthreshold = 200.0\n",
      "mean reward = 199.46000\tthreshold = 200.0\n",
      "mean reward = 199.23000\tthreshold = 200.0\n",
      "mean reward = 198.81000\tthreshold = 200.0\n",
      "mean reward = 199.63000\tthreshold = 200.0\n",
      "mean reward = 199.83000\tthreshold = 200.0\n",
      "mean reward = 199.39000\tthreshold = 200.0\n",
      "mean reward = 200.00000\tthreshold = 200.0\n"
     ]
    }
   ],
   "source": [
    "n_samples = 100\n",
    "percentile = 70\n",
    "smoothing = 0.01\n",
    "\n",
    "for i in range(100):\n",
    "    #generate new sessions\n",
    "    sessions = [generate_session() for _ in range(n_samples)]\n",
    "\n",
    "    batch_states,batch_actions,batch_rewards = map(np.array,zip(*sessions))\n",
    "    #batch_states: a list of lists of states in each session\n",
    "    #batch_actions: a list of lists of actions in each session\n",
    "    #batch_rewards: a list of floats - total rewards at each session\n",
    "\n",
    "    threshold = np.percentile(batch_rewards, percentile)\n",
    "    \n",
    "    elite_states = batch_states[batch_rewards >= threshold]\n",
    "    elite_actions = batch_actions[batch_rewards >= threshold]\n",
    "    \n",
    "    elite_states, elite_actions = map(np.concatenate,[elite_states,elite_actions])\n",
    "    #elite_states: a list of states from top games\n",
    "    #elite_actions: a list of actions from top games\n",
    "    \n",
    "    agent.fit(elite_states, elite_actions)\n",
    "\n",
    "    print(\"mean reward = %.5f\\tthreshold = %.1f\"%(np.mean(batch_rewards),threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-10 20:16:14,139] Creating monitor directory videos\n",
      "[2017-04-10 20:16:14,230] Starting new video recorder writing to /home/shmuma/gpu/work/Practical_RL/week1/videos/openaigym.video.0.18649.video000000.mp4\n",
      "[2017-04-10 20:16:16,489] Starting new video recorder writing to /home/shmuma/gpu/work/Practical_RL/week1/videos/openaigym.video.0.18649.video000001.mp4\n",
      "[2017-04-10 20:16:18,556] Starting new video recorder writing to /home/shmuma/gpu/work/Practical_RL/week1/videos/openaigym.video.0.18649.video000008.mp4\n",
      "[2017-04-10 20:16:20,866] Starting new video recorder writing to /home/shmuma/gpu/work/Practical_RL/week1/videos/openaigym.video.0.18649.video000027.mp4\n",
      "[2017-04-10 20:16:23,675] Starting new video recorder writing to /home/shmuma/gpu/work/Practical_RL/week1/videos/openaigym.video.0.18649.video000064.mp4\n",
      "[2017-04-10 20:16:26,461] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/gpu/work/Practical_RL/week1/videos')\n"
     ]
    }
   ],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(env,directory=\"videos\",force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()\n",
    "#unwrap \n",
    "env = env.env.env\n",
    "#upload to gym\n",
    "#gym.upload(\"./videos/\",api_key=\"<your_api_key>\") #you'll need me later\n",
    "\n",
    "#Warning! If you keep seeing error that reads something like\"DoubleWrapError\",\n",
    "#run env=gym.make(\"CartPole-v0\");env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./videos/openaigym.video.0.18649.video000064.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework part I\n",
    "\n",
    "### Tabular correntropy method\n",
    "\n",
    "You may have noticed that the taxi problem quickly converges from -10k to aroung -500 score (+- 500) and stays there. This is in part because taxi-v2 has some hard-coded randomness in the environment. Other reason is that the percentile was chosen poorly.\n",
    "\n",
    "### Tasks\n",
    "- __1.1__ (1 pt) Modify the tabular CEM (CrossEntropyMethod) code to plot distribution of rewards and threshold on each tick.\n",
    "- __1.2__ (2 pts) Find out how the algorithm performance changes if you change different percentile and different n_samples.\n",
    "\n",
    "```<YOUR ANSWER>```\n",
    "\n",
    "\n",
    "- __1.3__ (2 pts) Tune the algorithm to end up with positive average score.\n",
    "- __1.4 bonus__ (1 pt) Try to achieve a distribution where 25% or more samples score above +9.0\n",
    "- __1.5 bonus__ (2 pts) Solve and upload [Taxi-v1](https://gym.openai.com/envs/Taxi-v1) to the openai gym.\n",
    "\n",
    "It's okay to modify the existing code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework part II\n",
    "\n",
    "### Deep crossentropy method\n",
    "\n",
    "By this moment you should have got enough score on [CartPole-v0](https://gym.openai.com/envs/CartPole-v0) to consider it solved (see the link). It's time to upload the result and get to something harder.\n",
    "\n",
    "* if you have any trouble with CartPole-v0 and feel stuck, feel free to ask us or your peers for help.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "* __2.1__ Go to the [gym site](http://gym.openai.com/), register and obtain __api key__.\n",
    "* __2.2__ (1 pt) Upload your result to gym via gym.upload (see Results tab above, the line you need is commented)\n",
    "* __2.3__ (3 pts) Pick one of environments: MountainCar-v0 or LunarLander-v2 (or both) and solve it.\n",
    "  * For MountainCar, learn to finish it in __less than 180 steps__\n",
    "  * For LunarLander, learn to get reward of __at least +50__\n",
    "  * See the tips section below, it's kinda important.\n",
    "  \n",
    "  \n",
    "* __2.4__ (1+ pt) Devise a way to speed up training at least 2x against the default version\n",
    "  * Obvious improvement: use [joblib](https://www.google.com/search?client=ubuntu&channel=fs&q=joblib&ie=utf-8&oe=utf-8)\n",
    "  * Try re-using samples from 3-5 last iterations when computing threshold and training\n",
    "  * Experiment with amount of training iterations and learning rate of the neural network (see params)\n",
    "  \n",
    "  \n",
    "### Tips\n",
    "* Gym page: [mountaincar](https://gym.openai.com/envs/MountainCar-v0), [lunarlander](https://gym.openai.com/envs/LunarLander-v2)\n",
    "* Sessions for MountainCar may last for 10k+ ticks. Make sure ```t_max``` param is at least 10k.\n",
    " * Also it may be a good idea to cut rewards via \">\" and not \">=\". If 90% of your sessions get reward of -10k and 20% are better, than if you use percentile 20% as threshold, R >= threshold __fails cut off bad sessions__ whule R > threshold works alright.\n",
    "* _issue with gym_: Some versions of gym limit game time by 200 ticks. This will prevent cem training in most cases. Make sure your agent is able to play for the specified __t_max__, and if it isn't, try `env = gym.make(\"MountainCar-v0\").env` or otherwise get rid of TimeLimit wrapper.\n",
    "* If you use old _swig_ lib for LunarLander-v2, you may get an error. See this [issue](https://github.com/openai/gym/issues/100) for solution.\n",
    "* If it won't train it's a good idea to plot reward distribution and record sessions: they may give you some clue. If they don't, call course staff :)\n",
    "* 20-neuron network is probably not enough, feel free to experiment.\n",
    "* __Please upload the results to openai gym and send links to all submissions in the e-mail__\n",
    "\n",
    "### Bonus tasks\n",
    "\n",
    "* __2.5 bonus__ Try to find a network architecture and training params that solve __both__ environments above (_Points depend on implementation_)\n",
    "\n",
    "* __2.6 bonus__ Solve continuous action space task with `MLPRegressor` or similar.\n",
    "  * [MountainCarContinuous-v0](https://gym.openai.com/envs/MountainCarContinuous-v0), [LunarLanderContinuous-v2](https://gym.openai.com/envs/LunarLanderContinuous-v2) (4+ points if it works)\n",
    "  \n",
    "* __2.7 bonus__ Use any deep learning framework of your choice to implement policy-gradient (see lectures) on any of those envs (4 +1 per env):\n",
    "  * CartPole-v0\n",
    "  * MountainCar-v0\n",
    "  * LunarLander-v2\n",
    "  * See __tips on policy gradient__ below.\n",
    "  \n",
    "\n",
    "* __2.8 bonus__ take your favorite deep learning framework and try to get above random in [Atari Breakout](https://gym.openai.com/envs/Breakout-v0) with crossentropy method over a convolutional network.\n",
    "  * Expect at least +10 points if you get this up and running, no deadlines apply ! \n",
    "  * __See tips below on where to start, they're cruicially important__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips on policy gradient\n",
    "\n",
    "* The loss function is very similar to crossentropy method. You can get away with using rewards as  __sample_weights__.\n",
    "* If your algorithm converges to a poor strategy, try regularizing with entropy or just somehow prevent agent from picking actions deterministically (e.g. when probs = 0,0,1,0,0)\n",
    "* We will use `lasagne` later in the course so you can try to [learn it](http://lasagne.readthedocs.io/en/latest/user/tutorial.html).\n",
    "* If you don't want to mess with theano just yet, try [keras](https://keras.io/getting-started/sequential-model-guide/) or [mxnet](http://mxnet.io/tutorials/index.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Tips on atari breakout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There's all the pre-processing and tuning done for you in the code below\n",
    "* Once you got it working, it's probably a good idea to pre-train with autoencoder or something\n",
    "* We use last 4 frames as observations to account for ball velocity\n",
    "* The code below requires ```pip install Image``` and ```pip install gym[atari]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from breakout import make_breakout\n",
    "\n",
    "env = make_breakout()\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the initial state\n",
    "s = env.reset()\n",
    "print (s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#plot first observation. Only one frame\n",
    "plt.imshow(s.swapaxes(1,2).reshape(-1,64).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#next frame\n",
    "new_s,r,done, _ = env.step(env.action_space.sample())\n",
    "plt.imshow(new_s.swapaxes(1,2).reshape(-1,64).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#after 10 frames\n",
    "for _ in range(10):\n",
    "    new_s,r,done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "plt.imshow(new_s.swapaxes(1,2).reshape(-1,64).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "< tons of your code here or elsewhere >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
