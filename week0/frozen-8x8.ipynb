{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FrozenLake 8x8 solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake8x8-v0\")\n",
    "\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# constants, hyperparams\n",
    "episode_max_steps = 1000\n",
    "evaluate_n_times = 100\n",
    "crossover_p = 0.5\n",
    "mutation_p = 0.1\n",
    "decay_mutation = 0.99\n",
    "\n",
    "n_epoches = 100\n",
    "pool_size = 100\n",
    "n_crossovers = 50\n",
    "n_mutations = 50\n",
    "\n",
    "recording = True\n",
    "recording_dir = \"result\"\n",
    "\n",
    "if recording:\n",
    "    # shut up logger\n",
    "    gym.logger.setLevel(logging.ERROR)\n",
    "    env = wrappers.Monitor(env, recording_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_random_policy():\n",
    "    return np.random.choice(n_actions, size=n_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_reward(env, policy):\n",
    "    s = env.reset()\n",
    "    total_reward = 0\n",
    "    for _ in range(episode_max_steps):\n",
    "        s, reward, is_done, _ = env.step(policy[s])\n",
    "        total_reward += reward\n",
    "        if is_done:\n",
    "            break\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(policy, n_times=evaluate_n_times):\n",
    "    rewards = [sample_reward(env, policy) for _ in range(n_times)]\n",
    "    return float(np.mean(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crossover(policy1, policy2, p):\n",
    "    \"\"\"\n",
    "    for each state, with probability p take action from policy1, else policy2\n",
    "    \"\"\"\n",
    "    idx = np.random.rand(len(policy1)) < p\n",
    "    return np.choose(idx, [policy1, policy2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mutation(policy, p):\n",
    "    \"\"\"\n",
    "    for each state, with probability p replace action with random action\n",
    "    Tip: mutation can be written as crossover with random policy\n",
    "    \"\"\"\n",
    "    return crossover(get_random_policy(), policy, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization\n",
    "pool = [get_random_policy() for _ in range(pool_size)]\n",
    "scores = [evaluate(p) for p in pool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeef4dc59dfa46a9a603711476898446"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Best score: 0.05\n",
      "Epoch 1: Best score: 0.06\n",
      "Epoch 2: Best score: 0.06\n",
      "Epoch 3: Best score: 0.08\n",
      "Epoch 4: Best score: 0.08\n",
      "Epoch 5: Best score: 0.08\n",
      "Epoch 6: Best score: 0.18\n",
      "Epoch 7: Best score: 0.18\n",
      "Epoch 8: Best score: 0.18\n",
      "Epoch 9: Best score: 0.18\n",
      "Epoch 10: Best score: 0.26\n",
      "Epoch 11: Best score: 0.43\n",
      "Epoch 12: Best score: 0.43\n",
      "Epoch 13: Best score: 0.43\n",
      "Epoch 14: Best score: 0.67\n",
      "Epoch 15: Best score: 0.67\n",
      "Epoch 16: Best score: 0.67\n",
      "Epoch 17: Best score: 0.67\n",
      "Epoch 18: Best score: 0.67\n",
      "Epoch 19: Best score: 0.67\n",
      "Epoch 20: Best score: 0.67\n",
      "Epoch 21: Best score: 0.74\n",
      "Epoch 22: Best score: 0.74\n",
      "Epoch 23: Best score: 0.77\n",
      "Epoch 24: Best score: 0.77\n",
      "Epoch 25: Best score: 0.77\n",
      "Epoch 26: Best score: 0.85\n",
      "Epoch 27: Best score: 0.91\n",
      "Epoch 28: Best score: 0.92\n",
      "Epoch 29: Best score: 0.92\n",
      "Epoch 30: Best score: 0.92\n",
      "Epoch 31: Best score: 0.92\n",
      "Epoch 32: Best score: 0.92\n",
      "Epoch 33: Best score: 0.93\n",
      "Epoch 34: Best score: 0.93\n",
      "Epoch 35: Best score: 0.94\n",
      "Epoch 36: Best score: 0.94\n",
      "Epoch 37: Best score: 0.94\n",
      "Epoch 38: Best score: 0.94\n",
      "Epoch 39: Best score: 0.94\n",
      "Epoch 40: Best score: 0.94\n",
      "Epoch 41: Best score: 0.94\n",
      "Epoch 42: Best score: 0.94\n",
      "Epoch 43: Best score: 0.94\n",
      "Epoch 44: Best score: 0.94\n",
      "Epoch 45: Best score: 0.95\n",
      "Epoch 46: Best score: 0.95\n",
      "Epoch 47: Best score: 0.95\n",
      "Epoch 48: Best score: 0.96\n",
      "Epoch 49: Best score: 0.96\n",
      "Epoch 50: Best score: 0.96\n",
      "Epoch 51: Best score: 0.96\n",
      "Epoch 52: Best score: 0.96\n",
      "Epoch 53: Best score: 0.96\n",
      "Epoch 54: Best score: 0.96\n",
      "Epoch 55: Best score: 0.96\n",
      "Epoch 56: Best score: 0.96\n",
      "Epoch 57: Best score: 0.96\n",
      "Epoch 58: Best score: 0.96\n",
      "Epoch 59: Best score: 0.96\n",
      "Epoch 60: Best score: 0.96\n",
      "Epoch 61: Best score: 0.96\n",
      "Epoch 62: Best score: 0.96\n",
      "Epoch 63: Best score: 0.96\n",
      "Epoch 64: Best score: 0.96\n",
      "Epoch 65: Best score: 0.96\n",
      "Epoch 66: Best score: 0.96\n",
      "Epoch 67: Best score: 0.96\n",
      "Epoch 68: Best score: 0.97\n",
      "Epoch 69: Best score: 0.97\n",
      "Epoch 70: Best score: 0.97\n",
      "Epoch 71: Best score: 0.97\n",
      "Epoch 72: Best score: 0.97\n",
      "Epoch 73: Best score: 0.97\n",
      "Epoch 74: Best score: 0.97\n",
      "Epoch 75: Best score: 0.97\n",
      "Epoch 76: Best score: 0.97\n",
      "Epoch 77: Best score: 0.97\n",
      "Epoch 78: Best score: 0.97\n",
      "Epoch 79: Best score: 0.97\n",
      "Epoch 80: Best score: 0.97\n",
      "Epoch 81: Best score: 0.97\n",
      "Epoch 82: Best score: 0.97\n",
      "Epoch 83: Best score: 0.97\n",
      "Epoch 84: Best score: 0.97\n",
      "Epoch 85: Best score: 0.97\n",
      "Epoch 86: Best score: 0.97\n",
      "Epoch 87: Best score: 0.97\n",
      "Epoch 88: Best score: 0.97\n",
      "Epoch 89: Best score: 0.97\n",
      "Epoch 90: Best score: 0.97\n",
      "Epoch 91: Best score: 0.97\n",
      "Epoch 92: Best score: 0.97\n",
      "Epoch 93: Best score: 0.97\n",
      "Epoch 94: Best score: 0.97\n",
      "Epoch 95: Best score: 0.97\n",
      "Epoch 96: Best score: 1.00\n",
      "Epoch 97: Best score: 1.00\n",
      "Epoch 98: Best score: 1.00\n",
      "Epoch 99: Best score: 1.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(n_epoches)):\n",
    "    mutations = [mutation(random.choice(pool), mutation_p) \n",
    "                 for _ in range(n_mutations)]\n",
    "    crossovers = [crossover(random.choice(pool), random.choice(pool), crossover_p)\n",
    "                 for _ in range(n_crossovers)]\n",
    "    pool.extend(mutations)\n",
    "    scores.extend([evaluate(p) for p in mutations])\n",
    "    pool.extend(crossovers)\n",
    "    scores.extend([evaluate(p) for p in crossovers])\n",
    "\n",
    "    # truncate the pool\n",
    "    selected_indices = np.argsort(scores)[-pool_size:]\n",
    "    pool = [pool[i] for i in selected_indices]\n",
    "    scores = [scores[i] for i in selected_indices]\n",
    "\n",
    "    best_score = scores[-1]\n",
    "    print(\"Epoch %d: Best score: %.2f\" % (epoch, best_score))\n",
    "    \n",
    "    mutation_p *= decay_mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gym.openai.com/evaluations/eval_u01ZYLDmRbq5Z1rY6iPvLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
