{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate q-learning\n",
    "\n",
    "In this notebook you will teach a lasagne neural network to do Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Frameworks__ - we'll accept this homework in any deep learning framework. For example, it translates to TensorFlow almost line-to-line. However, we recommend you to stick to theano/lasagne unless you're certain about your skills in the framework of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Lambda\n",
    "from keras.optimizers import RMSprop, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pycrayon import CrayonClient\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = CrayonClient(hostname='localhost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-21 15:35:38,573] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_actions=2, state_dim=(4,)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\").env\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "print(\"n_actions={}, state_dim={}\".format(n_actions, state_dim))\n",
    "#plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate (deep) Q-learning: building the network\n",
    "\n",
    "In this section we will build and train naive Q-learning with theano/lasagne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is initializing input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L1_SIZE = 50\n",
    "L2_SIZE = 50\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "l1 (Dense)                   (None, 50)                250       \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 352\n",
      "Trainable params: 352\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# input - observation, output - qvalues\n",
    "in_t = Input(shape=state_dim, name='input')\n",
    "l1_t = Dense(L1_SIZE, activation='relu', name='l1')(in_t)\n",
    "#l2_t = Dense(L2_SIZE, activation='relu', name='l2')(l1_t)\n",
    "l_out_t = Dense(n_actions, name='out')(l1_t)\n",
    "model = Model(inputs=[in_t], outputs=[l_out_t])\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=Adam(lr=0.0005), loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crayon = client.create_experiment(\"test-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.25 #initial epsilon\n",
    "\n",
    "def generate_session(t_max=1000):\n",
    "    \"\"\"play env with approximate q-learning agent and train it at the same time\"\"\"\n",
    "    \n",
    "    total_reward = 0\n",
    "    s = env.reset()\n",
    "    q_vals_means = []\n",
    "\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        q_vals = model.predict_on_batch(np.array([s]))[0]\n",
    "        if np.random.rand() < epsilon:\n",
    "            a = env.action_space.sample()\n",
    "        else:\n",
    "            a = np.argmax(q_vals)\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "\n",
    "        new_q = model.predict_on_batch(np.array([new_s]))[0]\n",
    "        valid_q = np.array(q_vals)\n",
    "        if done:\n",
    "            valid_q[a] = r\n",
    "        else:\n",
    "            valid_q[a] = r + gamma * new_q.max()\n",
    " \n",
    "        batch_x.append(s)\n",
    "        batch_y.append(valid_q)\n",
    "        q_vals_means.append(q_vals.mean())\n",
    "        total_reward+=r\n",
    "        \n",
    "        s = new_s\n",
    "        if done: break\n",
    "\n",
    "    crayon.add_scalar_value(\"q_mean\", float(np.mean(q_vals_means)))\n",
    "    crayon.add_scalar_value(\"reward\", total_reward)\n",
    "\n",
    "    l = model.train_on_batch(np.array(batch_x), np.array(batch_y))\n",
    "    crayon.add_scalar_value(\"loss\", float(l))\n",
    "    \n",
    "    return total_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: mean reward:10.870\tepsilon:0.23750\n",
      "1: mean reward:10.750\tepsilon:0.22562\n",
      "2: mean reward:10.660\tepsilon:0.21434\n",
      "3: mean reward:10.340\tepsilon:0.20363\n",
      "4: mean reward:10.560\tepsilon:0.19345\n",
      "5: mean reward:10.260\tepsilon:0.18377\n",
      "6: mean reward:10.370\tepsilon:0.17458\n",
      "7: mean reward:10.460\tepsilon:0.16586\n",
      "8: mean reward:10.170\tepsilon:0.15756\n",
      "9: mean reward:10.280\tepsilon:0.14968\n",
      "10: mean reward:10.570\tepsilon:0.14220\n",
      "11: mean reward:10.950\tepsilon:0.13509\n",
      "12: mean reward:11.460\tepsilon:0.12834\n",
      "13: mean reward:12.010\tepsilon:0.12192\n",
      "14: mean reward:11.700\tepsilon:0.11582\n",
      "15: mean reward:12.460\tepsilon:0.11003\n",
      "16: mean reward:12.810\tepsilon:0.10453\n",
      "17: mean reward:12.060\tepsilon:0.09930\n",
      "18: mean reward:14.220\tepsilon:0.09434\n",
      "19: mean reward:13.110\tepsilon:0.08962\n",
      "20: mean reward:13.430\tepsilon:0.08514\n",
      "21: mean reward:14.150\tepsilon:0.08088\n",
      "22: mean reward:12.760\tepsilon:0.07684\n",
      "23: mean reward:15.390\tepsilon:0.07300\n",
      "24: mean reward:13.040\tepsilon:0.06935\n",
      "25: mean reward:16.260\tepsilon:0.06588\n",
      "26: mean reward:16.930\tepsilon:0.06259\n",
      "27: mean reward:13.440\tepsilon:0.05946\n",
      "28: mean reward:19.440\tepsilon:0.05648\n",
      "29: mean reward:14.650\tepsilon:0.05366\n",
      "30: mean reward:17.110\tepsilon:0.05098\n",
      "31: mean reward:16.100\tepsilon:0.04843\n",
      "32: mean reward:17.620\tepsilon:0.04601\n",
      "33: mean reward:16.580\tepsilon:0.04371\n",
      "34: mean reward:19.240\tepsilon:0.04152\n",
      "35: mean reward:17.030\tepsilon:0.03944\n",
      "36: mean reward:22.660\tepsilon:0.03747\n",
      "37: mean reward:17.480\tepsilon:0.03560\n",
      "38: mean reward:31.590\tepsilon:0.03382\n",
      "39: mean reward:21.090\tepsilon:0.03213\n",
      "40: mean reward:57.490\tepsilon:0.03052\n",
      "41: mean reward:81.040\tepsilon:0.02900\n",
      "42: mean reward:42.170\tepsilon:0.02755\n",
      "43: mean reward:44.050\tepsilon:0.02617\n",
      "44: mean reward:46.170\tepsilon:0.02486\n",
      "45: mean reward:45.650\tepsilon:0.02362\n",
      "46: mean reward:47.650\tepsilon:0.02244\n",
      "47: mean reward:51.200\tepsilon:0.02131\n",
      "48: mean reward:63.730\tepsilon:0.02025\n",
      "49: mean reward:69.090\tepsilon:0.01924\n",
      "50: mean reward:49.490\tepsilon:0.01827\n",
      "51: mean reward:59.500\tepsilon:0.01736\n",
      "52: mean reward:82.160\tepsilon:0.01649\n",
      "53: mean reward:42.460\tepsilon:0.01567\n",
      "54: mean reward:39.500\tepsilon:0.01488\n",
      "55: mean reward:89.920\tepsilon:0.01414\n",
      "56: mean reward:100.080\tepsilon:0.01343\n",
      "57: mean reward:182.600\tepsilon:0.01276\n",
      "58: mean reward:199.390\tepsilon:0.01212\n",
      "59: mean reward:238.760\tepsilon:0.01152\n",
      "60: mean reward:301.950\tepsilon:0.01094\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):   \n",
    "    rewards = [generate_session() for _ in range(100)] #generate new sessions\n",
    "    \n",
    "    epsilon*=0.95\n",
    "    \n",
    "    print (\"%d: mean reward:%.3f\\tepsilon:%.5f\"%(i, np.mean(rewards),epsilon))\n",
    "\n",
    "    if np.mean(rewards) > 300:\n",
    "        print (\"You Win!\")\n",
    "        break\n",
    "        \n",
    "    assert epsilon!=0, \"Please explore environment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon=0 #Don't forget to reset epsilon back to initial value if you want to go on training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(env,directory=\"videos\",force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()\n",
    "#unwrap \n",
    "env = env.env.env\n",
    "#upload to gym\n",
    "#gym.upload(\"./videos/\",api_key=\"<your_api_key>\") #you'll need me later\n",
    "\n",
    "#Warning! If you keep seeing error that reads something like\"DoubleWrapError\",\n",
    "#run env=gym.make(\"CartPole-v0\");env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
