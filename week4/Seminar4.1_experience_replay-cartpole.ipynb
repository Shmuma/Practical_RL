{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple q-learning agent with experience replay\n",
    "\n",
    "We re-write q-learning algorithm using _agentnet_ - a helper for lasagne that implements some RL techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS='floatX=float32'\n",
      "Starting virtual X frame buffer: Xvfb.\n",
      "env: DISPLAY=:1\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "%env THEANO_FLAGS='floatX=float32'\n",
    "\n",
    "#XVFB will be launched if you run on a server\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment setup\n",
    "* Here we simply load the game and check that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-01 19:06:23,335] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "make_env = lambda: gym.make(\"CartPole-v0\").env\n",
    "\n",
    "env=make_env()\n",
    "env.reset()\n",
    "\n",
    "state_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plt.imshow(env.render(\"rgb_array\"))\n",
    "#del env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "[2017-05-01 19:06:28,013] The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX 1080 (CNMeM is disabled, cuDNN 5105)\n"
     ]
    }
   ],
   "source": [
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "from lasagne.nonlinearities import elu\n",
    "\n",
    "\n",
    "#image observation at current tick goes here, shape = (sample_i,x,y,color)\n",
    "observation_layer = InputLayer((None,)+state_shape)\n",
    "\n",
    "\n",
    "nn = DenseLayer(observation_layer, 50, nonlinearity=elu)\n",
    "#nn = DenseLayer(nn, 200, nonlinearity=elu)\n",
    "\n",
    "#a layer that predicts Qvalues\n",
    "qvalues_layer = DenseLayer(nn,num_units=n_actions,\n",
    "                           nonlinearity=None,name=\"q-values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picking actions is done by yet another layer, that implements $ \\epsilon$ -greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from agentnet.resolver import EpsilonGreedyResolver\n",
    "action_layer = EpsilonGreedyResolver(qvalues_layer)\n",
    "\n",
    "#set starting epsilon\n",
    "action_layer.epsilon.set_value(np.float32(0.05))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent\n",
    "\n",
    "We define an agent entirely composed of a lasagne network:\n",
    "* Observations as InputLayer(s)\n",
    "* Actions as intermediate Layer(s)\n",
    "* `policy_estimators` is \"whatever else you want to keep track of\"\n",
    "\n",
    "Each parameter can be either one layer or a list of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from agentnet.agent import Agent\n",
    "agent = Agent(observation_layers=observation_layer,\n",
    "              action_layers=action_layer,\n",
    "              policy_estimators=qvalues_layer,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[W, b, q-values.W, q-values.b]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since it's a single lasagne network, one can get it's weights, output, etc\n",
    "weights = lasagne.layers.get_all_params(action_layer,trainable=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and manage a pool of atari sessions to play with\n",
    "\n",
    "* To make training more stable, we shall have an entire batch of game sessions each happening independent of others\n",
    "* Why several parallel agents help training: http://arxiv.org/pdf/1602.01783v1.pdf\n",
    "* Alternative approach: store more sessions: https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-01 19:06:36,696] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "from agentnet.experiments.openai_gym.pool import EnvPool\n",
    "pool = EnvPool(agent,make_env,n_games=1,max_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions: [[0 1 1 0 0]]\n",
      "rewards: [[ 1.  1.  1.  1.  0.]]\n",
      "CPU times: user 0 ns, sys: 4 ms, total: 4 ms\n",
      "Wall time: 2.71 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#interact for 7 ticks\n",
    "obs_log,action_log,reward_log,_,_,_  = pool.interact(5)\n",
    "\n",
    "\n",
    "print('actions:',action_log)\n",
    "print('rewards:',reward_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we'll train on rollouts of 10 steps (required by n-step algorithms and rnns later)\n",
    "SEQ_LENGTH=10\n",
    "\n",
    "#load first sessions (this function calls interact and stores sessions in the pool)\n",
    "\n",
    "for _ in range(100):\n",
    "    pool.update(SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# q-learning\n",
    "\n",
    "We shall now define a function that replays recent game sessions and updates network weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get agent's Qvalues obtained via experience replay\n",
    "replay = pool.experience_replay.sample_session_batch(100)\n",
    "qvalues_seq = agent.get_sessions(\n",
    "    replay,\n",
    "    session_length=SEQ_LENGTH,\n",
    "    experience_replay=True,\n",
    ")[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loss for Qlearning = (Q(s,a) - (r+gamma*Q(s',a_max)))^2, like you implemented before in lasagne.\n",
    "\n",
    "from agentnet.learning import qlearning\n",
    "elwise_mse_loss = qlearning.get_elementwise_objective(qvalues_seq,\n",
    "                                                      replay.actions[0],\n",
    "                                                      replay.rewards,\n",
    "                                                      replay.is_alive,\n",
    "                                                      gamma_or_gammas=0.99,\n",
    "                                                      n_steps=1,)\n",
    "\n",
    "#compute mean loss over \"alive\" fragments\n",
    "loss = elwise_mse_loss.sum() / replay.is_alive.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get weight updates\n",
    "updates = lasagne.updates.adam(loss,weights,learning_rate=1e-3)\n",
    "\n",
    "#compile train function\n",
    "import theano\n",
    "train_step = theano.function([],loss,updates=updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo run\n",
    "\n",
    "Play full session with an untrained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-01 19:06:45,230] Making new env: CartPole-v0\n",
      "[2017-05-01 19:06:45,232] Creating monitor directory ./records\n",
      "[2017-05-01 19:06:45,257] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 70 timesteps with reward=70.0\n"
     ]
    }
   ],
   "source": [
    "#for MountainCar-v0 evaluation session is cropped to 200 ticks\n",
    "untrained_reward = pool.evaluate(save_path=\"./records\",record_video=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-29 23:10:19,191] An unexpected error occurred while tokenizing input\n",
      "The following traceback may be corrupted or invalid\n",
      "The error message is: ('EOF in multi-line string', (1, 0))\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-0ade85e3459e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;34m<\u001b[0m\u001b[0msource\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"{}\"\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"video/mp4\"\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \"\"\".format(\"./records/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./records/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./records/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epoch_counter = 1 #starting epoch\n",
    "rewards = {} #full game rewards\n",
    "target_score = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 98/10000 [00:01<03:18, 49.84it/s][2017-05-01 19:06:55,061] Making new env: CartPole-v0\n",
      "[2017-05-01 19:06:55,063] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-01 19:06:55,080] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      "  1%|          | 108/10000 [00:02<03:27, 47.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 22 timesteps with reward=22.0\n",
      "Episode finished after 17 timesteps with reward=17.0\n",
      "Episode finished after 16 timesteps with reward=16.0\n",
      "iter=100\tepsilon=0.910\n",
      "Current score(mean over 3) = 18.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 198/10000 [00:04<03:21, 48.53it/s][2017-05-01 19:06:57,129] Making new env: CartPole-v0\n",
      "[2017-05-01 19:06:57,131] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-01 19:06:57,149] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      "  2%|▏         | 208/10000 [00:04<03:30, 46.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 25 timesteps with reward=25.0\n",
      "Episode finished after 19 timesteps with reward=19.0\n",
      "Episode finished after 18 timesteps with reward=18.0\n",
      "iter=200\tepsilon=0.828\n",
      "Current score(mean over 3) = 20.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 298/10000 [00:06<03:22, 47.91it/s][2017-05-01 19:06:59,231] Making new env: CartPole-v0\n",
      "[2017-05-01 19:06:59,233] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-01 19:06:59,253] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      "  3%|▎         | 308/10000 [00:06<03:32, 45.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 16 timesteps with reward=16.0\n",
      "Episode finished after 36 timesteps with reward=36.0\n",
      "Episode finished after 16 timesteps with reward=16.0\n",
      "iter=300\tepsilon=0.754\n",
      "Current score(mean over 3) = 22.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 398/10000 [00:08<03:46, 42.48it/s][2017-05-01 19:07:01,540] Making new env: CartPole-v0\n",
      "[2017-05-01 19:07:01,541] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-01 19:07:01,558] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      "  4%|▍         | 403/10000 [00:08<03:57, 40.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 13 timesteps with reward=13.0\n",
      "Episode finished after 15 timesteps with reward=15.0\n",
      "Episode finished after 27 timesteps with reward=27.0\n",
      "iter=400\tepsilon=0.687\n",
      "Current score(mean over 3) = 18.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 498/10000 [00:10<03:46, 41.87it/s][2017-05-01 19:07:03,936] Making new env: CartPole-v0\n",
      "[2017-05-01 19:07:03,938] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-01 19:07:03,978] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      "  5%|▌         | 503/10000 [00:10<04:11, 37.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 36 timesteps with reward=36.0\n",
      "Episode finished after 41 timesteps with reward=41.0\n",
      "Episode finished after 63 timesteps with reward=63.0\n",
      "iter=500\tepsilon=0.626\n",
      "Current score(mean over 3) = 46.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 598/10000 [00:13<03:48, 41.14it/s][2017-05-01 19:07:06,395] Making new env: CartPole-v0\n",
      "[2017-05-01 19:07:06,396] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-01 19:07:06,445] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      "  6%|▌         | 603/10000 [00:13<04:17, 36.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 74 timesteps with reward=74.0\n",
      "Episode finished after 76 timesteps with reward=76.0\n",
      "Episode finished after 17 timesteps with reward=17.0\n",
      "iter=600\tepsilon=0.571\n",
      "Current score(mean over 3) = 55.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 698/10000 [00:15<03:56, 39.36it/s][2017-05-01 19:07:08,942] Making new env: CartPole-v0\n",
      "[2017-05-01 19:07:08,944] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-01 19:07:08,997] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      "  7%|▋         | 702/10000 [00:15<04:34, 33.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 53 timesteps with reward=53.0\n",
      "Episode finished after 57 timesteps with reward=57.0\n",
      "Episode finished after 78 timesteps with reward=78.0\n",
      "iter=700\tepsilon=0.522\n",
      "Current score(mean over 3) = 62.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 799/10000 [00:18<03:50, 39.84it/s][2017-05-01 19:07:11,495] Making new env: CartPole-v0\n",
      "[2017-05-01 19:07:11,496] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-01 19:07:11,541] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      "  8%|▊         | 803/10000 [00:18<04:23, 34.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 60 timesteps with reward=60.0\n",
      "Episode finished after 62 timesteps with reward=62.0\n",
      "Episode finished after 33 timesteps with reward=33.0\n",
      "iter=800\tepsilon=0.477\n",
      "Current score(mean over 3) = 51.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 899/10000 [00:20<03:51, 39.25it/s][2017-05-01 19:07:14,078] Making new env: CartPole-v0\n",
      "[2017-05-01 19:07:14,080] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-01 19:07:14,117] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      "  9%|▉         | 903/10000 [00:21<04:18, 35.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 28 timesteps with reward=28.0\n",
      "Episode finished after 52 timesteps with reward=52.0\n",
      "Episode finished after 46 timesteps with reward=46.0\n",
      "iter=900\tepsilon=0.436\n",
      "Current score(mean over 3) = 42.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 999/10000 [00:23<03:51, 38.83it/s][2017-05-01 19:07:16,691] Making new env: CartPole-v0\n",
      "[2017-05-01 19:07:16,692] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-01 19:07:16,725] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      " 10%|█         | 1003/10000 [00:23<04:15, 35.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 26 timesteps with reward=26.0\n",
      "Episode finished after 39 timesteps with reward=39.0\n",
      "Episode finished after 50 timesteps with reward=50.0\n",
      "iter=1000\tepsilon=0.399\n",
      "Current score(mean over 3) = 38.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1099/10000 [00:26<03:53, 38.09it/s][2017-05-01 19:07:19,337] Making new env: CartPole-v0\n",
      "[2017-05-01 19:07:19,339] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-01 19:07:19,384] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      " 11%|█         | 1103/10000 [00:26<04:24, 33.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 56 timesteps with reward=56.0\n",
      "Episode finished after 59 timesteps with reward=59.0\n",
      "Episode finished after 40 timesteps with reward=40.0\n",
      "iter=1100\tepsilon=0.366\n",
      "Current score(mean over 3) = 51.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 1199/10000 [00:28<03:52, 37.77it/s][2017-05-01 19:07:22,023] Making new env: CartPole-v0\n",
      "[2017-05-01 19:07:22,025] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-01 19:07:22,062] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      " 12%|█▏        | 1203/10000 [00:29<04:19, 33.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 59 timesteps with reward=59.0\n",
      "Episode finished after 42 timesteps with reward=42.0\n",
      "Episode finished after 30 timesteps with reward=30.0\n",
      "iter=1200\tepsilon=0.336\n",
      "Current score(mean over 3) = 43.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 1299/10000 [00:31<03:54, 37.09it/s][2017-05-01 19:07:24,747] Making new env: CartPole-v0\n",
      "[2017-05-01 19:07:24,749] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-01 19:07:24,805] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      " 13%|█▎        | 1303/10000 [00:31<04:32, 31.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 55 timesteps with reward=55.0\n",
      "Episode finished after 82 timesteps with reward=82.0\n",
      "Episode finished after 62 timesteps with reward=62.0\n",
      "iter=1300\tepsilon=0.309\n",
      "Current score(mean over 3) = 66.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1396/10000 [00:33<03:19, 43.18it/s][2017-05-01 19:07:27,154] Making new env: CartPole-v0\n",
      "[2017-05-01 19:07:27,156] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-01 19:07:27,272] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      " 14%|█▍        | 1406/10000 [00:34<04:02, 35.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 269 timesteps with reward=269.0\n",
      "Episode finished after 72 timesteps with reward=72.0\n",
      "Episode finished after 76 timesteps with reward=76.0\n",
      "iter=1400\tepsilon=0.284\n",
      "Current score(mean over 3) = 139.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 1496/10000 [00:36<03:19, 42.62it/s][2017-05-01 19:07:29,612] Making new env: CartPole-v0\n",
      "[2017-05-01 19:07:29,613] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-01 19:07:29,694] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      " 15%|█▌        | 1506/10000 [00:36<03:50, 36.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 89 timesteps with reward=89.0\n",
      "Episode finished after 75 timesteps with reward=75.0\n",
      "Episode finished after 125 timesteps with reward=125.0\n",
      "iter=1500\tepsilon=0.262\n",
      "Current score(mean over 3) = 96.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 1596/10000 [00:38<03:21, 41.61it/s][2017-05-01 19:07:32,088] Making new env: CartPole-v0\n",
      "[2017-05-01 19:07:32,090] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-01 19:07:32,178] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      " 16%|█▌        | 1601/10000 [00:39<04:07, 33.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 102 timesteps with reward=102.0\n",
      "Episode finished after 93 timesteps with reward=93.0\n",
      "Episode finished after 121 timesteps with reward=121.0\n",
      "iter=1600\tepsilon=0.242\n",
      "Current score(mean over 3) = 105.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1696/10000 [00:41<03:22, 40.98it/s][2017-05-01 19:07:34,610] Making new env: CartPole-v0\n",
      "[2017-05-01 19:07:34,612] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-01 19:07:34,749] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      " 17%|█▋        | 1701/10000 [00:41<04:32, 30.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 244 timesteps with reward=244.0\n",
      "Episode finished after 116 timesteps with reward=116.0\n",
      "Episode finished after 130 timesteps with reward=130.0\n",
      "iter=1700\tepsilon=0.224\n",
      "Current score(mean over 3) = 163.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 1796/10000 [00:44<03:21, 40.66it/s][2017-05-01 19:07:37,199] Making new env: CartPole-v0\n",
      "[2017-05-01 19:07:37,201] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-01 19:07:37,331] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      " 18%|█▊        | 1801/10000 [00:44<04:27, 30.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 129 timesteps with reward=129.0\n",
      "Episode finished after 183 timesteps with reward=183.0\n",
      "Episode finished after 157 timesteps with reward=157.0\n",
      "iter=1800\tepsilon=0.207\n",
      "Current score(mean over 3) = 156.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 1895/10000 [00:46<03:22, 40.03it/s][2017-05-01 19:07:39,823] Making new env: CartPole-v0\n",
      "[2017-05-01 19:07:39,824] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-01 19:07:39,965] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      " 19%|█▉        | 1900/10000 [00:46<04:32, 29.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 130 timesteps with reward=130.0\n",
      "Episode finished after 226 timesteps with reward=226.0\n",
      "Episode finished after 142 timesteps with reward=142.0\n",
      "iter=1900\tepsilon=0.192\n",
      "Current score(mean over 3) = 166.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 1996/10000 [00:49<03:24, 39.22it/s][2017-05-01 19:07:42,501] Making new env: CartPole-v0\n",
      "[2017-05-01 19:07:42,502] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-01 19:07:42,648] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      " 20%|██        | 2005/10000 [00:49<04:23, 30.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 249 timesteps with reward=249.0\n",
      "Episode finished after 138 timesteps with reward=138.0\n",
      "Episode finished after 135 timesteps with reward=135.0\n",
      "iter=2000\tepsilon=0.179\n",
      "Current score(mean over 3) = 174.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 2095/10000 [00:51<03:13, 40.92it/s][2017-05-01 19:07:45,093] Making new env: CartPole-v0\n",
      "[2017-05-01 19:07:45,095] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-01 19:07:45,275] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      " 21%|██        | 2100/10000 [00:52<04:39, 28.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 245 timesteps with reward=245.0\n",
      "Episode finished after 192 timesteps with reward=192.0\n",
      "Episode finished after 208 timesteps with reward=208.0\n",
      "iter=2100\tepsilon=0.166\n",
      "Current score(mean over 3) = 215.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2195/10000 [00:54<03:10, 40.94it/s][2017-05-01 19:07:47,719] Making new env: CartPole-v0\n",
      "[2017-05-01 19:07:47,720] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-01 19:07:47,849] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      " 22%|██▏       | 2200/10000 [00:54<04:11, 30.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 154 timesteps with reward=154.0\n",
      "Episode finished after 159 timesteps with reward=159.0\n",
      "Episode finished after 146 timesteps with reward=146.0\n",
      "iter=2200\tepsilon=0.155\n",
      "Current score(mean over 3) = 153.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 2295/10000 [00:57<03:08, 40.95it/s][2017-05-01 19:07:50,296] Making new env: CartPole-v0\n",
      "[2017-05-01 19:07:50,297] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-01 19:07:50,427] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      " 23%|██▎       | 2300/10000 [00:57<04:09, 30.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 155 timesteps with reward=155.0\n",
      "Episode finished after 146 timesteps with reward=146.0\n",
      "Episode finished after 163 timesteps with reward=163.0\n",
      "iter=2300\tepsilon=0.145\n",
      "Current score(mean over 3) = 154.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 2395/10000 [00:59<03:04, 41.25it/s][2017-05-01 19:07:52,860] Making new env: CartPole-v0\n",
      "[2017-05-01 19:07:52,862] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-01 19:07:52,990] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      " 24%|██▍       | 2400/10000 [00:59<04:03, 31.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 148 timesteps with reward=148.0\n",
      "Episode finished after 155 timesteps with reward=155.0\n",
      "Episode finished after 156 timesteps with reward=156.0\n",
      "iter=2400\tepsilon=0.136\n",
      "Current score(mean over 3) = 153.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 2495/10000 [01:02<03:02, 41.18it/s][2017-05-01 19:07:55,417] Making new env: CartPole-v0\n",
      "[2017-05-01 19:07:55,418] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-01 19:07:55,569] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      " 25%|██▌       | 2500/10000 [01:02<04:11, 29.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 165 timesteps with reward=165.0\n",
      "Episode finished after 175 timesteps with reward=175.0\n",
      "Episode finished after 206 timesteps with reward=206.0\n",
      "iter=2500\tepsilon=0.128\n",
      "Current score(mean over 3) = 182.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 2595/10000 [01:04<03:00, 40.96it/s][2017-05-01 19:07:58,011] Making new env: CartPole-v0\n",
      "[2017-05-01 19:07:58,013] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-01 19:07:58,165] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      " 26%|██▌       | 2600/10000 [01:05<04:09, 29.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 177 timesteps with reward=177.0\n",
      "Episode finished after 189 timesteps with reward=189.0\n",
      "Episode finished after 182 timesteps with reward=182.0\n",
      "iter=2600\tepsilon=0.121\n",
      "Current score(mean over 3) = 182.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 2699/10000 [01:07<02:58, 40.94it/s][2017-05-01 19:08:00,612] Making new env: CartPole-v0\n",
      "[2017-05-01 19:08:00,614] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-01 19:08:00,780] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      " 27%|██▋       | 2704/10000 [01:07<04:12, 28.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 243 timesteps with reward=243.0\n",
      "Episode finished after 163 timesteps with reward=163.0\n",
      "Episode finished after 192 timesteps with reward=192.0\n",
      "iter=2700\tepsilon=0.114\n",
      "Current score(mean over 3) = 199.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 2799/10000 [01:10<02:56, 40.84it/s][2017-05-01 19:08:03,225] Making new env: CartPole-v0\n",
      "[2017-05-01 19:08:03,226] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 204 timesteps with reward=204.0\n",
      "Episode finished after 283 timesteps with reward=283.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-01 19:08:03,488] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 452 timesteps with reward=452.0\n",
      "iter=2800\tepsilon=0.108\n",
      "Current score(mean over 3) = 313.000\n",
      "You win!\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "for i in trange(10000):    \n",
    "    \n",
    "    #play\n",
    "    for _ in range(5):\n",
    "        pool.update(SEQ_LENGTH,append=True)\n",
    "    \n",
    "    #train\n",
    "    train_step()\n",
    "    \n",
    "    #update epsilon\n",
    "    epsilon = 0.05 + 0.95*np.exp(-epoch_counter/1000.)\n",
    "    action_layer.epsilon.set_value(np.float32(epsilon))\n",
    "    \n",
    "    #play a few games for evaluation\n",
    "    if epoch_counter%100==0:\n",
    "        rewards[epoch_counter] = np.mean(pool.evaluate(n_games=3,record_video=False))\n",
    "        print(\"iter=%i\\tepsilon=%.3f\"%(epoch_counter,action_layer.epsilon.get_value(),))\n",
    "        print(\"Current score(mean over %i) = %.3f\"%(3,np.mean(rewards[epoch_counter])))\n",
    "    \n",
    "        if rewards[epoch_counter] >= target_score:\n",
    "            print(\"You win!\")\n",
    "            break\n",
    "\n",
    "    \n",
    "    epoch_counter  +=1\n",
    "\n",
    "    \n",
    "# Time to drink some coffee!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.09115407, -0.15741515, -0.17688504, -0.83037353],\n",
       "       [-0.09430237, -0.34973556, -0.1934925 , -0.5981276 ],\n",
       "       [-0.10129709, -0.15250808, -0.20545505, -0.9449805 ],\n",
       "       [-0.10434724, -0.34435919, -0.22435467, -0.72323602],\n",
       "       [ 0.02989943, -0.03761347, -0.01013596, -0.00807199],\n",
       "       [ 0.02914716,  0.15765236, -0.01029739, -0.30393562],\n",
       "       [ 0.03230021, -0.03732133, -0.01637611, -0.01451796],\n",
       "       [ 0.03155379,  0.1580316 , -0.01666647, -0.31232244],\n",
       "       [ 0.03471442,  0.35338697, -0.02291291, -0.61021447],\n",
       "       [ 0.04178216,  0.15859267, -0.03511721, -0.32483554]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool.experience_replay.observations[0].get_value()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04178216,  0.15859267, -0.03511721, -0.32483554],\n",
       "       [ 0.04495401,  0.35419658, -0.04161391, -0.62838286],\n",
       "       [ 0.05203794,  0.15967935, -0.05418157, -0.34909058],\n",
       "       [ 0.05523153, -0.03463181, -0.06116338, -0.07397313],\n",
       "       [ 0.05453889,  0.16131122, -0.06264285, -0.38530892],\n",
       "       [ 0.05776512, -0.03286806, -0.07034902, -0.11301624],\n",
       "       [ 0.05710775,  0.16318771, -0.07260935, -0.42703804],\n",
       "       [ 0.06037151, -0.03083476, -0.08115011, -0.15809959],\n",
       "       [ 0.05975481,  0.1653496 , -0.0843121 , -0.47523966],\n",
       "       [ 0.0630618 ,  0.36155462, -0.0938169 , -0.79326016]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool.experience_replay.observations[0].get_value()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = pool.interact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shmuma/anaconda3/envs/pract_rl/lib/python3.5/site-packages/ipykernel_launcher.py:3: FutureWarning: pd.ewm_mean is deprecated for ndarrays and will be removed in a future version\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f929218e780>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n39MajQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9296c134e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "          \r",
      " 28%|██▊       | 2799/10000 [01:30<03:51, 31.07it/s]"
     ]
    }
   ],
   "source": [
    "from pandas import ewma\n",
    "iters,session_rewards=zip(*sorted(rewards.items(),key=lambda t: t[0]))\n",
    "plt.plot(iters,ewma(np.array(session_rewards),span=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_reward = pool.evaluate(n_games=10,save_path=\"./records\",record_video=True)\n",
    "\n",
    "print(\"average reward:\",final_reward)\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./records/\")))\n",
    "\n",
    "for video_name in video_names:\n",
    "    HTML(\"\"\"\n",
    "    <video width=\"640\" height=\"480\" controls>\n",
    "      <source src=\"{}\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    \"\"\".format(\"./records/\"+video_name)) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Homework part I (5+ pts)\n",
    "\n",
    "Train a neural network for [`LunarLander-v2`](https://gym.openai.com/envs/LunarLander-v2).\n",
    "* Getting average reward of at least +0 gets you 5 points\n",
    "* Higher reward = more points\n",
    "\n",
    "\n",
    "## Bonus I\n",
    "* Try getting the same [or better] results on Acrobot __(+2 pts)__ or __LunarLander (+3 pts)__ using on-policy methods\n",
    "* You can get n-step q-learning by messing with ```n_steps``` param in the q-learning code above\n",
    "* Note that using large experience replay buffer will slow down on-policy algorithms to almost zero, so it's probably a good idea to use small experience replay buffer with several parallel agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
