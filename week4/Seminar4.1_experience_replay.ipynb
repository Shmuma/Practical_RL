{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple q-learning agent with experience replay\n",
    "\n",
    "We re-write q-learning algorithm using _agentnet_ - a helper for lasagne that implements some RL techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS='floatX=float32'\n",
      "Starting virtual X frame buffer: Xvfb.\n",
      "env: DISPLAY=:1\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "%env THEANO_FLAGS='floatX=float32'\n",
    "\n",
    "#XVFB will be launched if you run on a server\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment setup\n",
    "* Here we simply load the game and check that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-29 23:10:00,115] Making new env: Acrobot-v1\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "make_env = lambda: gym.make(\"Acrobot-v1\")\n",
    "\n",
    "env=make_env()\n",
    "env.reset()\n",
    "\n",
    "state_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plt.imshow(env.render(\"rgb_array\"))\n",
    "#del env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "[2017-04-29 23:10:02,744] The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX 1080 (CNMeM is disabled, cuDNN 5105)\n"
     ]
    }
   ],
   "source": [
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "from lasagne.nonlinearities import elu\n",
    "\n",
    "\n",
    "#image observation at current tick goes here, shape = (sample_i,x,y,color)\n",
    "observation_layer = InputLayer((None,)+state_shape)\n",
    "\n",
    "\n",
    "nn = DenseLayer(observation_layer, 200, nonlinearity=elu)\n",
    "nn = DenseLayer(nn, 200, nonlinearity=elu)\n",
    "\n",
    "#a layer that predicts Qvalues\n",
    "qvalues_layer = DenseLayer(nn,num_units=n_actions,\n",
    "                           nonlinearity=None,name=\"q-values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picking actions is done by yet another layer, that implements $ \\epsilon$ -greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentnet.resolver import EpsilonGreedyResolver\n",
    "action_layer = EpsilonGreedyResolver(qvalues_layer)\n",
    "\n",
    "#set starting epsilon\n",
    "action_layer.epsilon.set_value(np.float32(0.05))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent\n",
    "\n",
    "We define an agent entirely composed of a lasagne network:\n",
    "* Observations as InputLayer(s)\n",
    "* Actions as intermediate Layer(s)\n",
    "* `policy_estimators` is \"whatever else you want to keep track of\"\n",
    "\n",
    "Each parameter can be either one layer or a list of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from agentnet.agent import Agent\n",
    "agent = Agent(observation_layers=observation_layer,\n",
    "              action_layers=action_layer,\n",
    "              policy_estimators=qvalues_layer,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[W, b, W, b, q-values.W, q-values.b]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since it's a single lasagne network, one can get it's weights, output, etc\n",
    "weights = lasagne.layers.get_all_params(action_layer,trainable=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and manage a pool of atari sessions to play with\n",
    "\n",
    "* To make training more stable, we shall have an entire batch of game sessions each happening independent of others\n",
    "* Why several parallel agents help training: http://arxiv.org/pdf/1602.01783v1.pdf\n",
    "* Alternative approach: store more sessions: https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-29 23:10:09,170] Making new env: Acrobot-v1\n"
     ]
    }
   ],
   "source": [
    "from agentnet.experiments.openai_gym.pool import EnvPool\n",
    "pool = EnvPool(agent,make_env,n_games=1,max_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions: [[0 0 2 0 0]]\n",
      "rewards: [[-1. -1. -1. -1.  0.]]\n",
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 3.48 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#interact for 7 ticks\n",
    "obs_log,action_log,reward_log,_,_,_  = pool.interact(5)\n",
    "\n",
    "\n",
    "print('actions:',action_log)\n",
    "print('rewards:',reward_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we'll train on rollouts of 10 steps (required by n-step algorithms and rnns later)\n",
    "SEQ_LENGTH=10\n",
    "\n",
    "#load first sessions (this function calls interact and stores sessions in the pool)\n",
    "\n",
    "for _ in range(100):\n",
    "    pool.update(SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# q-learning\n",
    "\n",
    "We shall now define a function that replays recent game sessions and updates network weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get agent's Qvalues obtained via experience replay\n",
    "replay = pool.experience_replay.sample_session_batch(100)\n",
    "qvalues_seq = agent.get_sessions(\n",
    "    replay,\n",
    "    session_length=SEQ_LENGTH,\n",
    "    experience_replay=True,\n",
    ")[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loss for Qlearning = (Q(s,a) - (r+gamma*Q(s',a_max)))^2, like you implemented before in lasagne.\n",
    "\n",
    "from agentnet.learning import qlearning\n",
    "elwise_mse_loss = qlearning.get_elementwise_objective(qvalues_seq,\n",
    "                                                      replay.actions[0],\n",
    "                                                      replay.rewards,\n",
    "                                                      replay.is_alive,\n",
    "                                                      gamma_or_gammas=0.99,\n",
    "                                                      n_steps=1,)\n",
    "\n",
    "#compute mean loss over \"alive\" fragments\n",
    "loss = elwise_mse_loss.sum() / replay.is_alive.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get weight updates\n",
    "updates = lasagne.updates.adam(loss,weights,learning_rate=1e-4)\n",
    "\n",
    "#compile train function\n",
    "import theano\n",
    "train_step = theano.function([],loss,updates=updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo run\n",
    "\n",
    "Play full session with an untrained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-29 23:10:18,157] Making new env: Acrobot-v1\n",
      "[2017-04-29 23:10:18,159] Clearing 3 monitor files from previous run (because force=True was provided)\n",
      "[2017-04-29 23:10:18,381] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 500 timesteps with reward=-500.0\n"
     ]
    }
   ],
   "source": [
    "#for MountainCar-v0 evaluation session is cropped to 200 ticks\n",
    "untrained_reward = pool.evaluate(save_path=\"./records\",record_video=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-29 23:10:19,191] An unexpected error occurred while tokenizing input\n",
      "The following traceback may be corrupted or invalid\n",
      "The error message is: ('EOF in multi-line string', (1, 0))\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-0ade85e3459e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;34m<\u001b[0m\u001b[0msource\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"{}\"\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"video/mp4\"\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \"\"\".format(\"./records/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./records/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./records/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epoch_counter = 1 #starting epoch\n",
    "rewards = {} #full game rewards\n",
    "target_score = -90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 96/10000 [00:02<04:29, 36.71it/s][2017-04-29 23:10:31,817] Making new env: Acrobot-v1\n",
      "[2017-04-29 23:10:31,819] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 500 timesteps with reward=-500.0\n",
      "Episode finished after 500 timesteps with reward=-500.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-29 23:10:32,489] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      "  1%|          | 104/10000 [00:03<10:19, 15.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 500 timesteps with reward=-500.0\n",
      "iter=100\tepsilon=0.910\n",
      "Current score(mean over 3) = -500.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 196/10000 [00:06<04:30, 36.21it/s][2017-04-29 23:10:35,242] Making new env: Acrobot-v1\n",
      "[2017-04-29 23:10:35,244] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 500 timesteps with reward=-500.0\n",
      "Episode finished after 500 timesteps with reward=-500.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-29 23:10:35,916] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      "  2%|▏         | 204/10000 [00:06<10:17, 15.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 500 timesteps with reward=-500.0\n",
      "iter=200\tepsilon=0.828\n",
      "Current score(mean over 3) = -500.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 296/10000 [00:09<04:54, 32.93it/s][2017-04-29 23:10:38,888] Making new env: Acrobot-v1\n",
      "[2017-04-29 23:10:38,890] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 477 timesteps with reward=-476.0\n",
      "Episode finished after 372 timesteps with reward=-371.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-29 23:10:39,485] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      "  3%|▎         | 304/10000 [00:10<09:58, 16.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 500 timesteps with reward=-500.0\n",
      "iter=300\tepsilon=0.754\n",
      "Current score(mean over 3) = -449.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 396/10000 [00:13<04:57, 32.25it/s][2017-04-29 23:10:42,566] Making new env: Acrobot-v1\n",
      "[2017-04-29 23:10:42,567] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 500 timesteps with reward=-500.0\n",
      "Episode finished after 422 timesteps with reward=-421.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-29 23:10:43,203] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      "  4%|▍         | 404/10000 [00:14<10:18, 15.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 500 timesteps with reward=-500.0\n",
      "iter=400\tepsilon=0.687\n",
      "Current score(mean over 3) = -473.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 496/10000 [00:17<04:59, 31.77it/s][2017-04-29 23:10:46,336] Making new env: Acrobot-v1\n",
      "[2017-04-29 23:10:46,338] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-04-29 23:10:46,588] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      "  5%|▌         | 500/10000 [00:17<07:58, 19.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 206 timesteps with reward=-205.0\n",
      "Episode finished after 179 timesteps with reward=-178.0\n",
      "Episode finished after 173 timesteps with reward=-172.0\n",
      "iter=500\tepsilon=0.626\n",
      "Current score(mean over 3) = -185.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 596/10000 [00:20<05:01, 31.23it/s][2017-04-29 23:10:49,774] Making new env: Acrobot-v1\n",
      "[2017-04-29 23:10:49,775] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 306 timesteps with reward=-305.0\n",
      "Episode finished after 295 timesteps with reward=-294.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-29 23:10:50,164] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      "  6%|▌         | 604/10000 [00:21<08:13, 19.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 260 timesteps with reward=-259.0\n",
      "iter=600\tepsilon=0.571\n",
      "Current score(mean over 3) = -286.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 696/10000 [00:24<05:00, 30.95it/s][2017-04-29 23:10:53,381] Making new env: Acrobot-v1\n",
      "[2017-04-29 23:10:53,383] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-04-29 23:10:53,606] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      "  7%|▋         | 700/10000 [00:24<07:37, 20.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 157 timesteps with reward=-156.0\n",
      "Episode finished after 149 timesteps with reward=-148.0\n",
      "Episode finished after 192 timesteps with reward=-191.0\n",
      "iter=700\tepsilon=0.522\n",
      "Current score(mean over 3) = -165.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 796/10000 [00:27<05:03, 30.31it/s][2017-04-29 23:10:56,891] Making new env: Acrobot-v1\n",
      "[2017-04-29 23:10:56,892] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-04-29 23:10:57,180] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      "  8%|▊         | 800/10000 [00:28<08:23, 18.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 216 timesteps with reward=-215.0\n",
      "Episode finished after 189 timesteps with reward=-188.0\n",
      "Episode finished after 238 timesteps with reward=-237.0\n",
      "iter=800\tepsilon=0.477\n",
      "Current score(mean over 3) = -213.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 899/10000 [00:31<05:05, 29.79it/s][2017-04-29 23:11:00,518] Making new env: Acrobot-v1\n",
      "[2017-04-29 23:11:00,520] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-04-29 23:11:00,751] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 175 timesteps with reward=-174.0\n",
      "Episode finished after 148 timesteps with reward=-147.0\n",
      "Episode finished after 188 timesteps with reward=-187.0\n",
      "iter=900\tepsilon=0.436\n",
      "Current score(mean over 3) = -169.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 998/10000 [00:34<05:04, 29.61it/s][2017-04-29 23:11:04,122] Making new env: Acrobot-v1\n",
      "[2017-04-29 23:11:04,124] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-04-29 23:11:04,315] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      " 10%|█         | 1001/10000 [00:35<07:59, 18.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 125 timesteps with reward=-124.0\n",
      "Episode finished after 173 timesteps with reward=-172.0\n",
      "Episode finished after 129 timesteps with reward=-128.0\n",
      "iter=1000\tepsilon=0.399\n",
      "Current score(mean over 3) = -141.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1097/10000 [00:38<05:05, 29.15it/s][2017-04-29 23:11:07,729] Making new env: Acrobot-v1\n",
      "[2017-04-29 23:11:07,731] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-04-29 23:11:07,960] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      " 11%|█         | 1100/10000 [00:38<08:31, 17.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 133 timesteps with reward=-132.0\n",
      "Episode finished after 199 timesteps with reward=-198.0\n",
      "Episode finished after 181 timesteps with reward=-180.0\n",
      "iter=1100\tepsilon=0.366\n",
      "Current score(mean over 3) = -170.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 1199/10000 [00:42<05:06, 28.74it/s][2017-04-29 23:11:11,426] Making new env: Acrobot-v1\n",
      "[2017-04-29 23:11:11,428] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-04-29 23:11:11,579] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      " 12%|█▏        | 1202/10000 [00:42<07:21, 19.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 100 timesteps with reward=-99.0\n",
      "Episode finished after 104 timesteps with reward=-103.0\n",
      "Episode finished after 135 timesteps with reward=-134.0\n",
      "iter=1200\tepsilon=0.336\n",
      "Current score(mean over 3) = -112.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 1298/10000 [00:45<05:07, 28.34it/s][2017-04-29 23:11:15,096] Making new env: Acrobot-v1\n",
      "[2017-04-29 23:11:15,098] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-04-29 23:11:15,357] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      " 13%|█▎        | 1301/10000 [00:46<08:53, 16.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 214 timesteps with reward=-213.0\n",
      "Episode finished after 151 timesteps with reward=-150.0\n",
      "Episode finished after 217 timesteps with reward=-216.0\n",
      "iter=1300\tepsilon=0.309\n",
      "Current score(mean over 3) = -193.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1397/10000 [00:49<04:55, 29.09it/s][2017-04-29 23:11:18,787] Making new env: Acrobot-v1\n",
      "[2017-04-29 23:11:18,789] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-04-29 23:11:18,942] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      " 14%|█▍        | 1403/10000 [00:49<06:29, 22.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 135 timesteps with reward=-134.0\n",
      "Episode finished after 99 timesteps with reward=-98.0\n",
      "Episode finished after 107 timesteps with reward=-106.0\n",
      "iter=1400\tepsilon=0.284\n",
      "Current score(mean over 3) = -112.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 1499/10000 [00:53<05:01, 28.20it/s][2017-04-29 23:11:22,423] Making new env: Acrobot-v1\n",
      "[2017-04-29 23:11:22,425] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-04-29 23:11:22,682] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 213 timesteps with reward=-212.0\n",
      "Episode finished after 189 timesteps with reward=-188.0\n",
      "Episode finished after 175 timesteps with reward=-174.0\n",
      "iter=1500\tepsilon=0.262\n",
      "Current score(mean over 3) = -191.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 1598/10000 [00:57<05:28, 25.60it/s][2017-04-29 23:11:26,578] Making new env: Acrobot-v1\n",
      "[2017-04-29 23:11:26,580] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-04-29 23:11:26,712] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      " 16%|█▌        | 1601/10000 [00:57<07:21, 19.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 98 timesteps with reward=-97.0\n",
      "Episode finished after 101 timesteps with reward=-100.0\n",
      "Episode finished after 94 timesteps with reward=-93.0\n",
      "iter=1600\tepsilon=0.242\n",
      "Current score(mean over 3) = -96.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1697/10000 [01:01<05:27, 25.38it/s][2017-04-29 23:11:30,640] Making new env: Acrobot-v1\n",
      "[2017-04-29 23:11:30,642] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-04-29 23:11:30,830] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      " 17%|█▋        | 1700/10000 [01:01<08:05, 17.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 153 timesteps with reward=-152.0\n",
      "Episode finished after 113 timesteps with reward=-112.0\n",
      "Episode finished after 157 timesteps with reward=-156.0\n",
      "iter=1700\tepsilon=0.224\n",
      "Current score(mean over 3) = -140.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 1799/10000 [01:05<04:58, 27.50it/s][2017-04-29 23:11:34,625] Making new env: Acrobot-v1\n",
      "[2017-04-29 23:11:34,627] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-04-29 23:11:34,938] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 294 timesteps with reward=-293.0\n",
      "Episode finished after 193 timesteps with reward=-192.0\n",
      "Episode finished after 218 timesteps with reward=-217.0\n",
      "iter=1800\tepsilon=0.207\n",
      "Current score(mean over 3) = -234.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 1898/10000 [01:09<04:57, 27.20it/s][2017-04-29 23:11:38,605] Making new env: Acrobot-v1\n",
      "[2017-04-29 23:11:38,607] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-04-29 23:11:38,853] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 99 timesteps with reward=-98.0\n",
      "Episode finished after 256 timesteps with reward=-255.0\n",
      "Episode finished after 189 timesteps with reward=-188.0\n",
      "iter=1900\tepsilon=0.192\n",
      "Current score(mean over 3) = -180.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 1997/10000 [01:13<04:59, 26.75it/s][2017-04-29 23:11:42,565] Making new env: Acrobot-v1\n",
      "[2017-04-29 23:11:42,567] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-04-29 23:11:42,698] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      " 20%|██        | 2003/10000 [01:13<06:09, 21.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 123 timesteps with reward=-122.0\n",
      "Episode finished after 86 timesteps with reward=-85.0\n",
      "Episode finished after 84 timesteps with reward=-83.0\n",
      "iter=2000\tepsilon=0.179\n",
      "Current score(mean over 3) = -96.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 2099/10000 [01:17<04:42, 27.97it/s][2017-04-29 23:11:46,275] Making new env: Acrobot-v1\n",
      "[2017-04-29 23:11:46,277] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-04-29 23:11:46,458] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      " 21%|██        | 2102/10000 [01:17<07:07, 18.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 157 timesteps with reward=-156.0\n",
      "Episode finished after 118 timesteps with reward=-117.0\n",
      "Episode finished after 133 timesteps with reward=-132.0\n",
      "iter=2100\tepsilon=0.166\n",
      "Current score(mean over 3) = -135.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2198/10000 [01:20<04:39, 27.91it/s][2017-04-29 23:11:50,043] Making new env: Acrobot-v1\n",
      "[2017-04-29 23:11:50,045] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-04-29 23:11:50,260] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      " 22%|██▏       | 2201/10000 [01:21<07:28, 17.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 223 timesteps with reward=-222.0\n",
      "Episode finished after 129 timesteps with reward=-128.0\n",
      "Episode finished after 131 timesteps with reward=-130.0\n",
      "iter=2200\tepsilon=0.155\n",
      "Current score(mean over 3) = -160.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 2297/10000 [01:24<04:36, 27.90it/s][2017-04-29 23:11:53,839] Making new env: Acrobot-v1\n",
      "[2017-04-29 23:11:53,841] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-04-29 23:11:54,077] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      " 23%|██▎       | 2300/10000 [01:24<07:39, 16.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 125 timesteps with reward=-124.0\n",
      "Episode finished after 171 timesteps with reward=-170.0\n",
      "Episode finished after 234 timesteps with reward=-233.0\n",
      "iter=2300\tepsilon=0.145\n",
      "Current score(mean over 3) = -175.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 2399/10000 [01:28<04:31, 27.95it/s][2017-04-29 23:11:57,658] Making new env: Acrobot-v1\n",
      "[2017-04-29 23:11:57,660] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-04-29 23:11:57,825] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      " 24%|██▍       | 2402/10000 [01:28<06:40, 18.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 89 timesteps with reward=-88.0\n",
      "Episode finished after 115 timesteps with reward=-114.0\n",
      "Episode finished after 167 timesteps with reward=-166.0\n",
      "iter=2400\tepsilon=0.136\n",
      "Current score(mean over 3) = -122.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 2498/10000 [01:32<04:28, 27.98it/s][2017-04-29 23:12:01,408] Making new env: Acrobot-v1\n",
      "[2017-04-29 23:12:01,410] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-04-29 23:12:01,596] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n",
      " 25%|██▌       | 2501/10000 [01:32<06:49, 18.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 108 timesteps with reward=-107.0\n",
      "Episode finished after 184 timesteps with reward=-183.0\n",
      "Episode finished after 120 timesteps with reward=-119.0\n",
      "iter=2500\tepsilon=0.128\n",
      "Current score(mean over 3) = -136.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 2597/10000 [01:35<04:27, 27.68it/s][2017-04-29 23:12:05,200] Making new env: Acrobot-v1\n",
      "[2017-04-29 23:12:05,201] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-04-29 23:12:05,305] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/shmuma/work/Practical_RL/week4/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 77 timesteps with reward=-76.0\n",
      "Episode finished after 77 timesteps with reward=-76.0\n",
      "Episode finished after 77 timesteps with reward=-76.0\n",
      "iter=2600\tepsilon=0.121\n",
      "Current score(mean over 3) = -76.000\n",
      "You win!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "          \r",
      " 26%|██▌       | 2597/10000 [01:50<05:13, 23.59it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "for i in trange(10000):    \n",
    "    \n",
    "    #play\n",
    "    for _ in range(5):\n",
    "        pool.update(SEQ_LENGTH,append=True)\n",
    "    \n",
    "    #train\n",
    "    train_step()\n",
    "    \n",
    "    #update epsilon\n",
    "    epsilon = 0.05 + 0.95*np.exp(-epoch_counter/1000.)\n",
    "    action_layer.epsilon.set_value(np.float32(epsilon))\n",
    "    \n",
    "    #play a few games for evaluation\n",
    "    if epoch_counter%100==0:\n",
    "        rewards[epoch_counter] = np.mean(pool.evaluate(n_games=3,record_video=False))\n",
    "        print(\"iter=%i\\tepsilon=%.3f\"%(epoch_counter,action_layer.epsilon.get_value(),))\n",
    "        print(\"Current score(mean over %i) = %.3f\"%(3,np.mean(rewards[epoch_counter])))\n",
    "    \n",
    "        if rewards[epoch_counter] >= target_score:\n",
    "            print(\"You win!\")\n",
    "            break\n",
    "\n",
    "    \n",
    "    epoch_counter  +=1\n",
    "\n",
    "    \n",
    "# Time to drink some coffee!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shmuma/anaconda3/envs/pract_rl/lib/python3.5/site-packages/ipykernel_launcher.py:3: FutureWarning: pd.ewm_mean is deprecated for ndarrays and will be removed in a future version\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe5bc2e42b0>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD8CAYAAACVZ8iyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe5c40c0710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pandas import ewma\n",
    "iters,session_rewards=zip(*sorted(rewards.items(),key=lambda p:p[0]))\n",
    "plt.plot(iters,ewma(np.array(session_rewards),span=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_reward = pool.evaluate(n_games=10,save_path=\"./records\",record_video=True)\n",
    "\n",
    "print(\"average reward:\",final_reward)\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./records/\")))\n",
    "\n",
    "for video_name in video_names:\n",
    "    HTML(\"\"\"\n",
    "    <video width=\"640\" height=\"480\" controls>\n",
    "      <source src=\"{}\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    \"\"\".format(\"./records/\"+video_name)) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Homework part I (5+ pts)\n",
    "\n",
    "Train a neural network for [`LunarLander-v2`](https://gym.openai.com/envs/LunarLander-v2).\n",
    "* Getting average reward of at least +0 gets you 5 points\n",
    "* Higher reward = more points\n",
    "\n",
    "\n",
    "## Bonus I\n",
    "* Try getting the same [or better] results on Acrobot __(+2 pts)__ or __LunarLander (+3 pts)__ using on-policy methods\n",
    "* You can get n-step q-learning by messing with ```n_steps``` param in the q-learning code above\n",
    "* Note that using large experience replay buffer will slow down on-policy algorithms to almost zero, so it's probably a good idea to use small experience replay buffer with several parallel agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
